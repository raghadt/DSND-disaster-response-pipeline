{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['punkt', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "df = pd.read_sql_table('InsertTableName', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.loc[df['related'] > 1, :].index, axis=0)\n",
    "categories = df.columns[-36:]\n",
    "X = df['message'].values\n",
    "Y = df[categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens_list = []\n",
    "    for token in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(token).lower().strip()\n",
    "        tokens_list.append(clean_tok)\n",
    "        \n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A Long-expected Party. The Shadow of the Past. Three is Company. A Short Cut to Mushrooms. A Conspiracy Unmasked. The Old Forest. In the House of Tom Bombadil. Fog on the Barrow-Downs. At the Sign of The Prancing Pony Strider. A Knife in the Dark .Flight to the Ford\"\n",
    "tokens_list = tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline ([\n",
    "    \n",
    "    ('veto', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.pipeline.Pipeline"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.82      0.93      0.88      4997\n",
      "               request       0.83      0.37      0.51      1090\n",
      "                 offer       0.00      0.00      0.00        38\n",
      "           aid_related       0.76      0.53      0.62      2739\n",
      "          medical_help       0.54      0.09      0.15       539\n",
      "      medical_products       0.70      0.12      0.21       320\n",
      "     search_and_rescue       0.44      0.02      0.04       187\n",
      "              security       0.00      0.00      0.00       125\n",
      "              military       0.62      0.06      0.11       219\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.80      0.21      0.33       426\n",
      "                  food       0.86      0.43      0.58       728\n",
      "               shelter       0.85      0.30      0.45       597\n",
      "              clothing       0.87      0.12      0.21       108\n",
      "                 money       0.50      0.03      0.05       155\n",
      "        missing_people       0.00      0.00      0.00        74\n",
      "              refugees       0.43      0.04      0.07       220\n",
      "                 death       0.75      0.15      0.25       271\n",
      "             other_aid       0.51      0.03      0.05       866\n",
      "infrastructure_related       0.38      0.01      0.01       422\n",
      "             transport       0.67      0.06      0.11       288\n",
      "             buildings       0.71      0.06      0.11       326\n",
      "           electricity       0.88      0.05      0.09       142\n",
      "                 tools       0.00      0.00      0.00        37\n",
      "             hospitals       0.00      0.00      0.00        68\n",
      "                 shops       0.00      0.00      0.00        27\n",
      "           aid_centers       0.00      0.00      0.00        87\n",
      "  other_infrastructure       0.00      0.00      0.00       278\n",
      "       weather_related       0.85      0.52      0.64      1813\n",
      "                floods       0.93      0.24      0.38       520\n",
      "                 storm       0.76      0.26      0.38       614\n",
      "                  fire       1.00      0.01      0.03        73\n",
      "            earthquake       0.91      0.58      0.71       621\n",
      "                  cold       0.44      0.04      0.07       112\n",
      "         other_weather       0.56      0.03      0.05       343\n",
      "         direct_report       0.78      0.34      0.47      1229\n",
      "\n",
      "           avg / total       0.74      0.45      0.51     20699\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names= y_test.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x7f5d42fcd950>, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1))], 'veto': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function tokenize at 0x7f5d42fcd950>, vocabulary=None), 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=1), 'veto__analyzer': 'word', 'veto__binary': False, 'veto__decode_error': 'strict', 'veto__dtype': <class 'numpy.int64'>, 'veto__encoding': 'utf-8', 'veto__input': 'content', 'veto__lowercase': True, 'veto__max_df': 1.0, 'veto__max_features': None, 'veto__min_df': 1, 'veto__ngram_range': (1, 1), 'veto__preprocessor': None, 'veto__stop_words': None, 'veto__strip_accents': None, 'veto__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'veto__tokenizer': <function tokenize at 0x7f5d42fcd950>, 'veto__vocabulary': None, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': True, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True, 'clf__estimator__bootstrap': True, 'clf__estimator__class_weight': None, 'clf__estimator__criterion': 'gini', 'clf__estimator__max_depth': None, 'clf__estimator__max_features': 'auto', 'clf__estimator__max_leaf_nodes': None, 'clf__estimator__min_impurity_decrease': 0.0, 'clf__estimator__min_impurity_split': None, 'clf__estimator__min_samples_leaf': 1, 'clf__estimator__min_samples_split': 2, 'clf__estimator__min_weight_fraction_leaf': 0.0, 'clf__estimator__n_estimators': 10, 'clf__estimator__n_jobs': 1, 'clf__estimator__oob_score': False, 'clf__estimator__random_state': None, 'clf__estimator__verbose': 0, 'clf__estimator__warm_start': False, 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'clf__n_jobs': 1}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__ngram_range': ((1, 1), (1, 2)),\n",
       " 'vect__max_df': (0.5, 0.75, 1.0),\n",
       " 'vect__max_features': (None, 5000),\n",
       " 'tfidf__use_idf': (True, False),\n",
       " 'clf__estimator__n_estimators': [10, 20],\n",
       " 'clf__estimator__min_samples_split': [2, 3]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters =  {\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'vect__max_features': (None, 5000),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'clf__estimator__n_estimators': [10, 20],\n",
    "        'clf__estimator__min_samples_split': [2, 3]\n",
    "    }\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=20,\n",
       "       param_grid={'vect__ngram_range': ((1, 1), (1, 2)), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000), 'tfidf__use_idf': (True, False), 'clf__estimator__n_estimators': [10, 20], 'clf__estimator__min_samples_split': [2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring=None, verbose=200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=200, return_train_score=False, n_jobs=20)\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to new file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038534996152/27-140038625428704-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__min_samples_split=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/conda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/conda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f5d87fd3d20, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/conda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f5d87fd3d20, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/conda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    492         if self.poller is not None:\n    493             self.poller.start()\n    494         self.kernel.start()\n    495         self.io_loop = ioloop.IOLoop.current()\n    496         try:\n--> 497             self.io_loop.start()\n        self.io_loop.start = <bound method PollIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    498         except KeyboardInterrupt:\n    499             pass\n    500 \n    501 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warning(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'fabc005d6ce54caf979cda4f0a215ba6']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'fabc005d6ce54caf979cda4f0a215ba6'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='cv.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'cv.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('cv.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('cv.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='cv.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = 'cv.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='cv.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-35-6c976e20f457>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>)\n   2902                     return True\n   2903 \n   2904             for i, node in enumerate(to_run_interactive):\n   2905                 mod = ast.Interactive([node])\n   2906                 code = compiler(mod, cell_name, \"single\")\n-> 2907                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>\n        result = <ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>\n   2908                     return True\n   2909 \n   2910             # Flush softspace\n   2911             if softspace(sys.stdout, 0):\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>, result=<ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>)\n   2956         outflag = True  # happens in more places, so it's easier as default\n   2957         try:\n   2958             try:\n   2959                 self.hooks.pre_run_code_hook()\n   2960                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2961                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', '# import libraries\\nimport pandas as pd\\nfrom sqla...from sklearn.metrics import classification_report', \"nltk.download(['punkt', 'wordnet'])\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", 'df.head(2)', \"categories = df.columns[-36:]\\nX = df['message'].values\\nY = df[categories]\", 'def tokenize(text):\\n    tokens = word_tokenize(t...append(clean_tok)\\n        \\n    return tokens_list', 'text = \"A Long-expected Party. The Shadow of the....Flight to the Ford\"\\ntokens_list = tokenize(text)', \"pipeline = Pipeline ([\\n    \\n    ('veto', CountVe...ltiOutputClassifier(RandomForestClassifier()))\\n])\", 'type(pipeline)', 'pipeline', 'X_train, X_test, y_train, y_test = train_test_split(X, Y)', '\\npipeline.fit(X_train, y_train)', 'y_pred = pipeline.predict(X_test)\\n\\nprint(classif..., y_pred, target_names= y_test.columns.tolist()))', 'y_pred = pipeline.predict(X_test)', 'print(classification_report(y_test, y_pred, target_names= y_test.columns.tolist()))', \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"\\n# load data from database\\nengine = create_engin...f = pd.read_sql_table('ResponseCategory', engine)\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", ...], 'MultiOutputClassifier': <class 'sklearn.multioutput.MultiOutputClassifier'>, 'Out': {2: True, 4:    id                                           ...        0              0  \n\n[2 rows x 40 columns], 9: <class 'sklearn.pipeline.Pipeline'>, 10: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 12: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 24: <class 'sklearn.pipeline.Pipeline'>, 25: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 27: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 33: {'clf__estimator__min_samples_split': [2, 3], 'clf__estimator__n_estimators': [10, 20], 'tfidf__use_idf': (True, False), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000), 'vect__ngram_range': ((1, 1), (1, 2))}, 34: GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200)}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', '# import libraries\\nimport pandas as pd\\nfrom sqla...from sklearn.metrics import classification_report', \"nltk.download(['punkt', 'wordnet'])\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", 'df.head(2)', \"categories = df.columns[-36:]\\nX = df['message'].values\\nY = df[categories]\", 'def tokenize(text):\\n    tokens = word_tokenize(t...append(clean_tok)\\n        \\n    return tokens_list', 'text = \"A Long-expected Party. The Shadow of the....Flight to the Ford\"\\ntokens_list = tokenize(text)', \"pipeline = Pipeline ([\\n    \\n    ('veto', CountVe...ltiOutputClassifier(RandomForestClassifier()))\\n])\", 'type(pipeline)', 'pipeline', 'X_train, X_test, y_train, y_test = train_test_split(X, Y)', '\\npipeline.fit(X_train, y_train)', 'y_pred = pipeline.predict(X_test)\\n\\nprint(classif..., y_pred, target_names= y_test.columns.tolist()))', 'y_pred = pipeline.predict(X_test)', 'print(classification_report(y_test, y_pred, target_names= y_test.columns.tolist()))', \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"\\n# load data from database\\nengine = create_engin...f = pd.read_sql_table('ResponseCategory', engine)\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", ...], 'MultiOutputClassifier': <class 'sklearn.multioutput.MultiOutputClassifier'>, 'Out': {2: True, 4:    id                                           ...        0              0  \n\n[2 rows x 40 columns], 9: <class 'sklearn.pipeline.Pipeline'>, 10: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 12: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 24: <class 'sklearn.pipeline.Pipeline'>, 25: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 27: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 33: {'clf__estimator__min_samples_split': [2, 3], 'clf__estimator__n_estimators': [10, 20], 'tfidf__use_idf': (True, False), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000), 'vect__ngram_range': ((1, 1), (1, 2))}, 34: GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200)}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n   2962             finally:\n   2963                 # Reset our crash handler in place\n   2964                 sys.excepthook = old_excepthook\n   2965         except SystemExit as e:\n\n...........................................................................\n/home/workspace/<ipython-input-35-6c976e20f457> in <module>()\n----> 1 cv.fit(X_train, y_train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=3, random_state=None, shuffle=False)>\n        X = array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object)\n        y =        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns]\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=20), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=20)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Jan 10 11:35:46 2020\nPID: 46                                 Python 3.6.3: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], scorer={'score': <function _passthrough_scorer>}, train=array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), test=array([   0,    1,    2, ..., 6504, 6505, 6506]), verbose=200, parameters={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        parameters = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **kwargs={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...      warm_start=False),\n           n_jobs=1))])>\n        kwargs = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), attr='steps', **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        params = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip...oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n           n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 444, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\", line 142, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\", line 49, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/base.py\", line 274, in set_params\n    (key, self))\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip...oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n           n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Jan 10 11:35:46 2020\nPID: 46                                 Python 3.6.3: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], scorer={'score': <function _passthrough_scorer>}, train=array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), test=array([   0,    1,    2, ..., 6504, 6505, 6506]), verbose=200, parameters={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        parameters = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **kwargs={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...      warm_start=False),\n           n_jobs=1))])>\n        kwargs = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), attr='steps', **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        params = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip...oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n           n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Jan 10 11:35:46 2020\nPID: 46                                 Python 3.6.3: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], scorer={'score': <function _passthrough_scorer>}, train=array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), test=array([   0,    1,    2, ..., 6504, 6505, 6506]), verbose=200, parameters={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        parameters = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **kwargs={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...      warm_start=False),\n           n_jobs=1))])>\n        kwargs = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), attr='steps', **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        params = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip...oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n           n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6c976e20f457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/opt/conda/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/opt/conda/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f5d87fd3d20, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/conda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f5d87fd3d20, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/opt/conda/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    492         if self.poller is not None:\n    493             self.poller.start()\n    494         self.kernel.start()\n    495         self.io_loop = ioloop.IOLoop.current()\n    496         try:\n--> 497             self.io_loop.start()\n        self.io_loop.start = <bound method PollIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    498         except KeyboardInterrupt:\n    499             pass\n    500 \n    501 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    445             return\n    446         zmq_events = self.socket.EVENTS\n    447         try:\n    448             # dispatch events:\n    449             if zmq_events & zmq.POLLIN and self.receiving():\n--> 450                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    451                 if not self.socket:\n    452                     return\n    453             if zmq_events & zmq.POLLOUT and self.sending():\n    454                 self._handle_send()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    475             else:\n    476                 raise\n    477         else:\n    478             if self._recv_callback:\n    479                 callback = self._recv_callback\n--> 480                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    481         \n    482 \n    483     def _handle_send(self):\n    484         \"\"\"Handle a send event.\"\"\"\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    427         close our socket.\"\"\"\n    428         try:\n    429             # Use a NullContext to ensure that all StackContexts are run\n    430             # inside our blanket exception handler rather than outside.\n    431             with stack_context.NullContext():\n--> 432                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    433         except:\n    434             gen_log.error(\"Uncaught exception in ZMQStream callback\",\n    435                           exc_info=True)\n    436             # Re-raise the exception so that IOLoop.handle_callback_exception\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warning(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'fabc005d6ce54caf979cda4f0a215ba6']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'fabc005d6ce54caf979cda4f0a215ba6'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'cv.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2020, 1, 10, 11, 35, 46, 100482, tzinfo=tzlocal()), 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'session': 'fabc005d6ce54caf979cda4f0a215ba6', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '7e1eb0d5f2a74a1a9cbb108f06dd9f2d', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='cv.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'cv.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('cv.fit(X_train, y_train)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('cv.fit(X_train, y_train)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='cv.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2657         -------\n   2658         result : :class:`ExecutionResult`\n   2659         \"\"\"\n   2660         try:\n   2661             result = self._run_cell(\n-> 2662                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = 'cv.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2663         finally:\n   2664             self.events.trigger('post_execute')\n   2665             if not silent:\n   2666                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='cv.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2780                 self.displayhook.exec_result = result\n   2781 \n   2782                 # Execute the user code\n   2783                 interactivity = 'none' if silent else self.ast_node_interactivity\n   2784                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2785                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2786                 \n   2787                 self.last_execution_succeeded = not has_raised\n   2788                 self.last_execution_result = result\n   2789 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-35-6c976e20f457>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>)\n   2902                     return True\n   2903 \n   2904             for i, node in enumerate(to_run_interactive):\n   2905                 mod = ast.Interactive([node])\n   2906                 code = compiler(mod, cell_name, \"single\")\n-> 2907                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>\n        result = <ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>\n   2908                     return True\n   2909 \n   2910             # Flush softspace\n   2911             if softspace(sys.stdout, 0):\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>, result=<ExecutionResult object at 7f5d4871f278, executi...rue silent=False shell_futures=True> result=None>)\n   2956         outflag = True  # happens in more places, so it's easier as default\n   2957         try:\n   2958             try:\n   2959                 self.hooks.pre_run_code_hook()\n   2960                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2961                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f5d42ea3300, file \"<ipython-input-35-6c976e20f457>\", line 1>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', '# import libraries\\nimport pandas as pd\\nfrom sqla...from sklearn.metrics import classification_report', \"nltk.download(['punkt', 'wordnet'])\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", 'df.head(2)', \"categories = df.columns[-36:]\\nX = df['message'].values\\nY = df[categories]\", 'def tokenize(text):\\n    tokens = word_tokenize(t...append(clean_tok)\\n        \\n    return tokens_list', 'text = \"A Long-expected Party. The Shadow of the....Flight to the Ford\"\\ntokens_list = tokenize(text)', \"pipeline = Pipeline ([\\n    \\n    ('veto', CountVe...ltiOutputClassifier(RandomForestClassifier()))\\n])\", 'type(pipeline)', 'pipeline', 'X_train, X_test, y_train, y_test = train_test_split(X, Y)', '\\npipeline.fit(X_train, y_train)', 'y_pred = pipeline.predict(X_test)\\n\\nprint(classif..., y_pred, target_names= y_test.columns.tolist()))', 'y_pred = pipeline.predict(X_test)', 'print(classification_report(y_test, y_pred, target_names= y_test.columns.tolist()))', \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"\\n# load data from database\\nengine = create_engin...f = pd.read_sql_table('ResponseCategory', engine)\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", ...], 'MultiOutputClassifier': <class 'sklearn.multioutput.MultiOutputClassifier'>, 'Out': {2: True, 4:    id                                           ...        0              0  \n\n[2 rows x 40 columns], 9: <class 'sklearn.pipeline.Pipeline'>, 10: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 12: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 24: <class 'sklearn.pipeline.Pipeline'>, 25: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 27: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 33: {'clf__estimator__min_samples_split': [2, 3], 'clf__estimator__n_estimators': [10, 20], 'tfidf__use_idf': (True, False), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000), 'vect__ngram_range': ((1, 1), (1, 2))}, 34: GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200)}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'DecisionTreeClassifier': <class 'sklearn.tree.tree.DecisionTreeClassifier'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', '# import libraries\\nimport pandas as pd\\nfrom sqla...from sklearn.metrics import classification_report', \"nltk.download(['punkt', 'wordnet'])\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", 'df.head(2)', \"categories = df.columns[-36:]\\nX = df['message'].values\\nY = df[categories]\", 'def tokenize(text):\\n    tokens = word_tokenize(t...append(clean_tok)\\n        \\n    return tokens_list', 'text = \"A Long-expected Party. The Shadow of the....Flight to the Ford\"\\ntokens_list = tokenize(text)', \"pipeline = Pipeline ([\\n    \\n    ('veto', CountVe...ltiOutputClassifier(RandomForestClassifier()))\\n])\", 'type(pipeline)', 'pipeline', 'X_train, X_test, y_train, y_test = train_test_split(X, Y)', '\\npipeline.fit(X_train, y_train)', 'y_pred = pipeline.predict(X_test)\\n\\nprint(classif..., y_pred, target_names= y_test.columns.tolist()))', 'y_pred = pipeline.predict(X_test)', 'print(classification_report(y_test, y_pred, target_names= y_test.columns.tolist()))', \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"# load data from database\\nengine = create_engine...pd.read_sql_table('ResponseCategory', con=engine)\", \"\\n# load data from database\\nengine = create_engin...f = pd.read_sql_table('ResponseCategory', engine)\", \"# load data from database\\nengine = create_engine... pd.read_sql_table('InsertTableName', con=engine)\", ...], 'MultiOutputClassifier': <class 'sklearn.multioutput.MultiOutputClassifier'>, 'Out': {2: True, 4:    id                                           ...        0              0  \n\n[2 rows x 40 columns], 9: <class 'sklearn.pipeline.Pipeline'>, 10: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 12: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 24: <class 'sklearn.pipeline.Pipeline'>, 25: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 27: Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), 33: {'clf__estimator__min_samples_split': [2, 3], 'clf__estimator__n_estimators': [10, 20], 'tfidf__use_idf': (True, False), 'vect__max_df': (0.5, 0.75, 1.0), 'vect__max_features': (None, 5000), 'vect__ngram_range': ((1, 1), (1, 2))}, 34: GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200)}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, ...}\n   2962             finally:\n   2963                 # Reset our crash handler in place\n   2964                 sys.excepthook = old_excepthook\n   2965         except SystemExit as e:\n\n...........................................................................\n/home/workspace/<ipython-input-35-6c976e20f457> in <module>()\n----> 1 cv.fit(X_train, y_train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...in_score=False,\n       scoring=None, verbose=200), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=3, random_state=None, shuffle=False)>\n        X = array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object)\n        y =        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns]\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=20), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=20)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Jan 10 11:35:46 2020\nPID: 46                                 Python 3.6.3: /opt/conda/bin/python\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object),        related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], {'score': <function _passthrough_scorer>}, array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), array([   0,    1,    2, ..., 6504, 6505, 6506]), 200, {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': False}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), X=array([ 'Damaged health facilities must start tr...y they don't have monney to pay \"], dtype=object), y=       related  request  offer  aid_related  med...    0              0  \n\n[19521 rows x 36 columns], scorer={'score': <function _passthrough_scorer>}, train=array([ 6507,  6508,  6509, ..., 19518, 19519, 19520]), test=array([   0,    1,    2, ..., 6504, 6505, 6506]), verbose=200, parameters={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score=False, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        parameters = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **kwargs={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...      warm_start=False),\n           n_jobs=1))])>\n        kwargs = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), attr='steps', **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...      warm_start=False),\n           n_jobs=1))])>\n        params = {'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\n/opt/conda/lib/python3.6/site-packages/sklearn/base.py in set_params(self=Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))]), **params={'clf__estimator__min_samples_split': 2, 'clf__estimator__n_estimators': 10, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__max_features': None, 'vect__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vect'\n        self = Pipeline(memory=None,\n     steps=[('veto', Count...       warm_start=False),\n           n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vect for estimator Pipeline(memory=None,\n     steps=[('veto', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n        strip...oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n           n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...er='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline_improved = Pipeline([\n",
    "    \n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier(DecisionTreeClassifier())))\n",
    "])\n",
    "pipline_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline_improved.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_improved = {\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'vect__max_features': (None, 5000, 10000),\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'clf__estimator__n_estimators': [10, 20],\n",
    "        'clf__estimator__base_estimator__max_depth': [2,5]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to new file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.24573536191793452, total=13.8min\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed: 13.8min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.22821576763485477, total=13.9min\n",
      "[Parallel(n_jobs=20)]: Done   2 tasks      | elapsed: 13.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.23697556477639464, total=13.9min\n",
      "[Parallel(n_jobs=20)]: Done   3 tasks      | elapsed: 13.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.24650376517596434, total=14.7min\n",
      "[Parallel(n_jobs=20)]: Done   4 tasks      | elapsed: 14.8min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.23559243891194098, total=14.8min\n",
      "[Parallel(n_jobs=20)]: Done   5 tasks      | elapsed: 14.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.239588135853696, total=14.8min\n",
      "[Parallel(n_jobs=20)]: Done   6 tasks      | elapsed: 14.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.24404487475026895, total=16.5min\n",
      "[Parallel(n_jobs=20)]: Done   7 tasks      | elapsed: 16.5min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.22883049024127863, total=16.6min\n",
      "[Parallel(n_jobs=20)]: Done   8 tasks      | elapsed: 16.6min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=None, vect__ngram_range=(1, 1), score=0.23205778392500384, total=16.6min\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl[Parallel(n_jobs=20)]: Done   9 tasks      | elapsed: 16.6min\n",
      "\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.24112494236975565, total=17.2min\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed: 17.3min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.23251882587982173, total=17.3min\n",
      "[Parallel(n_jobs=20)]: Done  11 tasks      | elapsed: 17.4min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.24727216843399416, total=17.4min\n",
      "[Parallel(n_jobs=20)]: Done  12 tasks      | elapsed: 17.5min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.23651452282157676, total=17.5min\n",
      "[Parallel(n_jobs=20)]: Done  13 tasks      | elapsed: 17.5min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.23835869064084833, total=17.5min\n",
      "[Parallel(n_jobs=20)]: Done  14 tasks      | elapsed: 17.6min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.24573536191793452, total=19.5min\n",
      "[Parallel(n_jobs=20)]: Done  15 tasks      | elapsed: 19.6min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.2346703550023052, total=19.6min\n",
      "[Parallel(n_jobs=20)]: Done  16 tasks      | elapsed: 19.6min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.5, vect__max_features=10000, vect__ngram_range=(1, 2), score=0.22990625480252036, total=19.6min\n",
      "[Parallel(n_jobs=20)]: Done  17 tasks      | elapsed: 19.7min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.24235438758260336, total=14.5min\n",
      "[Parallel(n_jobs=20)]: Done  18 tasks      | elapsed: 29.4min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.23897341324727217, total=14.5min\n",
      "[Parallel(n_jobs=20)]: Done  19 tasks      | elapsed: 29.4min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 1), score=0.23052097740894423, total=14.6min\n",
      "[Parallel(n_jobs=20)]: Done  20 tasks      | elapsed: 31.1min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=None, vect__ngram_range=(1, 1), score=0.23251882587982173, total=17.4min\n",
      "[Parallel(n_jobs=20)]: Done  21 tasks      | elapsed: 31.2min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.2400491778085139, total=15.5min\n",
      "[Parallel(n_jobs=20)]: Done  22 tasks      | elapsed: 32.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=None, vect__ngram_range=(1, 2) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.2483479329952359, total=15.4min\n",
      "[Parallel(n_jobs=20)]: Done  23 tasks      | elapsed: 32.9min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=10000, vect__ngram_range=(1, 1), score=0.22606423851237128, total=15.5min\n",
      "[Parallel(n_jobs=20)]: Done  24 tasks      | elapsed: 33.1min\n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.24312279084063315, total=18.1min\n",
      "[Parallel(n_jobs=20)]: Done  25 tasks      | elapsed: 34.7min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 1) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.23497771630551714, total=18.2min\n",
      "[Parallel(n_jobs=20)]: Done  26 tasks      | elapsed: 34.8min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n",
      "[CV]  clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=0.75, vect__max_features=5000, vect__ngram_range=(1, 2), score=0.2346703550023052, total=18.2min\n",
      "[Parallel(n_jobs=20)]: Done  27 tasks      | elapsed: 35.6min\n",
      "[CV] clf__estimator__base_estimator__max_depth=2, clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__max_df=1.0, vect__max_features=5000, vect__ngram_range=(1, 2) \n",
      "Pickling array (shape=(19521,), dtype=object).\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(19521,), dtype=int64).\n",
      "Memmaping (shape=(36, 19521), dtype=int64) to old file /dev/shm/joblib_memmaping_pool_27_140038647742816/27-140038517632864-d51b906e7010cbcd2749253b42dcf53f.pkl\n",
      "Pickling array (shape=(36,), dtype=object).\n",
      "Pickling array (shape=(13014,), dtype=int64).\n",
      "Pickling array (shape=(6507,), dtype=int64).\n"
     ]
    }
   ],
   "source": [
    "cv2 = GridSearchCV(pipline_improved, param_grid=parameters_improved, verbose=200, return_train_score=False, n_jobs=20)\n",
    "\n",
    "cv2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_imp = cv2.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_imp, target_names= y_test.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classifier.pkl\", 'wb') as file:\n",
    "    pickle.dump(cv, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
